{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:02:54.832347Z",
     "iopub.status.busy": "2025-04-18T16:02:54.832180Z",
     "iopub.status.idle": "2025-04-18T16:03:25.128741Z",
     "shell.execute_reply": "2025-04-18T16:03:25.127879Z",
     "shell.execute_reply.started": "2025-04-18T16:02:54.832330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:03:25.130017Z",
     "iopub.status.busy": "2025-04-18T16:03:25.129543Z",
     "iopub.status.idle": "2025-04-18T16:03:25.137085Z",
     "shell.execute_reply": "2025-04-18T16:03:25.136363Z",
     "shell.execute_reply.started": "2025-04-18T16:03:25.129994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed(42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:03:25.139036Z",
     "iopub.status.busy": "2025-04-18T16:03:25.138682Z",
     "iopub.status.idle": "2025-04-18T16:03:40.682173Z",
     "shell.execute_reply": "2025-04-18T16:03:40.681318Z",
     "shell.execute_reply.started": "2025-04-18T16:03:25.139019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"thainq107/iwslt2015-en-vi\")\n",
    "train_data, valid_data, test_data = ds[\"train\"], ds[\"validation\"], ds[\"test\"]\n",
    "en_lengths = [len(sentence) for sentence in train_data[\"en\"]]\n",
    "vi_lengths = [len(sentence) for sentence in train_data[\"vi\"]]\n",
    "\n",
    "combined_lengths = en_lengths + vi_lengths\n",
    "\n",
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = np.percentile(combined_lengths, 25)\n",
    "Q3 = np.percentile(combined_lengths, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "MAX_LENGTH = int(Q3 + 1.5*IQR)\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:03:40.683278Z",
     "iopub.status.busy": "2025-04-18T16:03:40.683001Z",
     "iopub.status.idle": "2025-04-18T16:03:45.482059Z",
     "shell.execute_reply": "2025-04-18T16:03:45.481402Z",
     "shell.execute_reply.started": "2025-04-18T16:03:40.683253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "unk_token = \"[UNK]\"\n",
    "pad_token = \"[PAD]\"\n",
    "mask_token = \"[MASK]\" # Try out\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "special_tokens = [unk_token, pad_token, sos_token, eos_token]\n",
    "\n",
    "en_tokenizer = Tokenizer(BPE(unk_token=unk_token))\n",
    "en_tokenizer.pre_tokenizer = Whitespace()\n",
    "en_trainer = BpeTrainer(special_tokens=special_tokens)\n",
    "en_tokenizer.train_from_iterator(train_data[\"en\"], trainer=en_trainer)\n",
    "\n",
    "vi_tokenizer = Tokenizer(BPE(unk_token=unk_token))\n",
    "vi_tokenizer.pre_tokenizer = Whitespace()\n",
    "vi_trainer = BpeTrainer(special_tokens=special_tokens)\n",
    "vi_tokenizer.train_from_iterator(train_data[\"vi\"], trainer=vi_trainer)\n",
    "\n",
    "# Build reverse lookup maps for later decoding (id -> token)\n",
    "def id_to_token(tokenizer):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    return {v: k for k, v in vocab.items()}\n",
    "    \n",
    "# For decoder\n",
    "en_id_to_token = id_to_token(en_tokenizer)\n",
    "vi_id_to_token = id_to_token(vi_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:03:45.486060Z",
     "iopub.status.busy": "2025-04-18T16:03:45.485830Z",
     "iopub.status.idle": "2025-04-18T16:03:45.511683Z",
     "shell.execute_reply": "2025-04-18T16:03:45.511101Z",
     "shell.execute_reply.started": "2025-04-18T16:03:45.486043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(en_tokenizer.get_vocab())   \n",
    "OUTPUT_DIM = len(vi_tokenizer.get_vocab())    \n",
    "EMBEDDING_DIM = 256 \n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 512            \n",
    "N_LAYERS = 2\n",
    "ENCODER_DROPOUT = 0.2\n",
    "DECODER_DROPOUT = 0.2\n",
    "BIDIRECTIONAL = False\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:03:45.512523Z",
     "iopub.status.busy": "2025-04-18T16:03:45.512312Z",
     "iopub.status.idle": "2025-04-18T16:04:12.157553Z",
     "shell.execute_reply": "2025-04-18T16:04:12.157007Z",
     "shell.execute_reply.started": "2025-04-18T16:03:45.512506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize(data, tokenizer, max_length=MAX_LENGTH):\n",
    "    encoding = tokenizer.encode(data)\n",
    "\n",
    "    ids = [tokenizer.token_to_id(sos_token)] + encoding.ids + [tokenizer.token_to_id(eos_token)]\n",
    "    return ids\n",
    "\n",
    "def tokenize_and_numericalize(data, src_tokenizer, trg_tokenizer, max_length=MAX_LENGTH):\n",
    "    return {\"src_ids\": tokenize(data[\"en\"], src_tokenizer),\n",
    "            \"trg_ids\": tokenize(data[\"vi\"], trg_tokenizer)    \n",
    "           }\n",
    "\n",
    "train_data = train_data.map(lambda x: tokenize_and_numericalize(x, en_tokenizer, vi_tokenizer))\n",
    "valid_data = valid_data.map(lambda x: tokenize_and_numericalize(x, en_tokenizer, vi_tokenizer))\n",
    "test_data = test_data.map(lambda x: tokenize_and_numericalize(x, en_tokenizer, vi_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:04:12.158562Z",
     "iopub.status.busy": "2025-04-18T16:04:12.158244Z",
     "iopub.status.idle": "2025-04-18T16:04:12.166622Z",
     "shell.execute_reply": "2025-04-18T16:04:12.166019Z",
     "shell.execute_reply.started": "2025-04-18T16:04:12.158513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_collate_fn(src_pad_index, trg_pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_src_ids = [torch.tensor(example[\"src_ids\"]) for example in batch]\n",
    "        batch_trg_ids = [torch.tensor(example[\"trg_ids\"]) for example in batch]\n",
    "        \n",
    "        # Manually pad the sequences to MAX_LENGTH (ensure truncation if necessary)\n",
    "        for i in range(len(batch_src_ids)):\n",
    "            # Truncate source sequences that are longer than MAX_LENGTH\n",
    "            if len(batch_src_ids[i]) > MAX_LENGTH:\n",
    "                batch_src_ids[i] = batch_src_ids[i][:MAX_LENGTH]\n",
    "            # Pad source sequences that are shorter than MAX_LENGTH\n",
    "            elif len(batch_src_ids[i]) < MAX_LENGTH:\n",
    "                batch_src_ids[i] = torch.cat([batch_src_ids[i], torch.full((MAX_LENGTH - len(batch_src_ids[i]),), src_pad_index)])\n",
    "            \n",
    "            # Truncate target sequences that are longer than MAX_LENGTH\n",
    "            if len(batch_trg_ids[i]) > MAX_LENGTH:\n",
    "                batch_trg_ids[i] = batch_trg_ids[i][:MAX_LENGTH]\n",
    "            # Pad target sequences that are shorter than MAX_LENGTH\n",
    "            elif len(batch_trg_ids[i]) < MAX_LENGTH:\n",
    "                batch_trg_ids[i] = torch.cat([batch_trg_ids[i], torch.full((MAX_LENGTH - len(batch_trg_ids[i]),), trg_pad_index)])\n",
    "\n",
    "        # Stack all the sequences to create the batch\n",
    "        batch_src_ids = torch.stack(batch_src_ids)\n",
    "        batch_trg_ids = torch.stack(batch_trg_ids)\n",
    "\n",
    "        return {\"src_ids\": batch_src_ids, \"trg_ids\": batch_trg_ids}\n",
    "    \n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "def get_data_loader(dataset, batch_size, src_pad_index, trg_pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(src_pad_index, trg_pad_index)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle)\n",
    "\n",
    "\n",
    "src_pad_index = en_tokenizer.token_to_id(pad_token)\n",
    "trg_pad_index = vi_tokenizer.token_to_id(pad_token)\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, BATCH_SIZE, src_pad_index, trg_pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, BATCH_SIZE, src_pad_index, trg_pad_index)\n",
    "test_data_loader  = get_data_loader(test_data,  BATCH_SIZE, src_pad_index, trg_pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:09:23.443924Z",
     "iopub.status.busy": "2025-04-18T16:09:23.443419Z",
     "iopub.status.idle": "2025-04-18T16:09:23.460696Z",
     "shell.execute_reply": "2025-04-18T16:09:23.459983Z",
     "shell.execute_reply.started": "2025-04-18T16:09:23.443899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for batch in train_data_loader:\n",
    "#     print(batch[\"trg_ids\"])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:31.668393Z",
     "iopub.status.busy": "2025-04-18T13:54:31.668181Z",
     "iopub.status.idle": "2025-04-18T13:54:31.966666Z",
     "shell.execute_reply": "2025-04-18T13:54:31.966000Z",
     "shell.execute_reply.started": "2025-04-18T13:54:31.668371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers=2, dropout_p=0.1, bidirectional=False, device='cuda'):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout_p)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.device = device\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        x = x.to(self.device)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        out, (hidden, cell) = self.lstm(embedding)\n",
    "        out = self.layer_norm(out)\n",
    "\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = hidden[-2,:,:] + hidden[-1,:,:]\n",
    "            out = self.fc(out)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        # out: [batch_size, seq_len, hidden_size]\n",
    "        # hidden: [batch_size, hidden_size]\n",
    "        return out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:31.969372Z",
     "iopub.status.busy": "2025-04-18T13:54:31.969107Z",
     "iopub.status.idle": "2025-04-18T13:54:31.981707Z",
     "shell.execute_reply": "2025-04-18T13:54:31.981135Z",
     "shell.execute_reply.started": "2025-04-18T13:54:31.969349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "       \n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)  # For the query (decoder hidden state)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)  # For the keys (encoder outputs)\n",
    "        self.Va = nn.Linear(hidden_size, 1)            # Final layer for the attention score\n",
    "\n",
    "    def forward(self, keys, query):\n",
    "\n",
    "        query_transformed = self.Wa(query).unsqueeze(1)  # Shape: [batch_size, 1, hidden_size]\n",
    "        keys_transformed = self.Ua(keys)  # Shape: [batch_size, seq_len, hidden_size]\n",
    "\n",
    "       \n",
    "        scores = self.Va(torch.tanh(query_transformed + keys_transformed))  # Shape: [batch_size, seq_len, 1]\n",
    "\n",
    "        scores = scores.squeeze(2)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "      \n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), keys)  # Shape: [batch_size, 1, hidden_size]\n",
    "\n",
    "        context_vector = context_vector.squeeze(1)  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:31.982718Z",
     "iopub.status.busy": "2025-04-18T13:54:31.982475Z",
     "iopub.status.idle": "2025-04-18T13:54:32.001237Z",
     "shell.execute_reply": "2025-04-18T13:54:32.000677Z",
     "shell.execute_reply.started": "2025-04-18T13:54:31.982697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        # hidden: [batch_size, hidden_size]\n",
    "\n",
    "        # Reshape hidden to be [batch_size, hidden_size, 1]\n",
    "        hidden = hidden.unsqueeze(2)  # [batch_size, hidden_size, 1]\n",
    "\n",
    "        # Compute attention scores (dot product)\n",
    "        attn_energies = torch.bmm(encoder_outputs, hidden) / (self.attn.in_features ** 0.5)  # [batch_size, seq_len, 1], Scaled dot-product attention\n",
    "        attn_energies = attn_energies.squeeze(2)  # [batch_size, seq_len]\n",
    "\n",
    "        # Compute the attention weights (probabilities)\n",
    "        attention_weights = torch.softmax(attn_energies, dim=1)  # [batch_size, seq_len]\n",
    "\n",
    "        # Compute the weighted sum of encoder outputs (context vector)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, hidden_size]\n",
    "        context = context.squeeze(1)  # [batch_size, hidden_size]\n",
    "\n",
    "        return context, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.002133Z",
     "iopub.status.busy": "2025-04-18T13:54:32.001884Z",
     "iopub.status.idle": "2025-04-18T13:54:32.018102Z",
     "shell.execute_reply": "2025-04-18T13:54:32.017587Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.002109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, vocab_size, embedding_dim, attention_type, dropout_p=0.1, n_layers=1, bidirectional=False):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        if attention_type == 'Luong':\n",
    "            self.attention = LuongAttention(hidden_size)\n",
    "        elif attention_type == 'Bahdanau':\n",
    "            self.attention = BahdanauAttention(hidden_size)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown attention type\")\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,  \n",
    "            hidden_size=self.hidden_size,  \n",
    "            num_layers=n_layers,  \n",
    "            batch_first=True,  \n",
    "            bidirectional=bidirectional,  \n",
    "            dropout=dropout_p\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size * self.n_directions + hidden_size , hidden_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn_lstm = nn.BatchNorm1d(hidden_size * self.n_directions)\n",
    "        self.bn_fc = nn.BatchNorm1d(hidden_size + hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        input: [batch_size]\n",
    "        hidden: [batch_size, hidden_size]\n",
    "        encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "\n",
    "        input = input.unsqueeze(1).to(DEVICE)\n",
    "        embedded = self.dropout(self.embedding(input))  # [batch_size, 1, embedding_dim]\n",
    "\n",
    "        if len(hidden.shape) == 2:\n",
    "            hidden = hidden.unsqueeze(0).repeat(self.n_layers * self.n_directions, 1, 1) # [, batch_size, hidden_size]\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, hidden)  # [batch_size, 1, hidden_size]\n",
    "\n",
    "        lstm_output = self.bn_lstm(lstm_output.squeeze(1))  # [batch_size, hidden_size]\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = hidden[-2, :, :] + hidden[-1, :, :]  \n",
    "            lstm_output = self.fc(lstm_output)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]  \n",
    "\n",
    "        context, attention_weights = self.attention(encoder_outputs, hidden) # \n",
    "\n",
    "        output = torch.cat([lstm_output, context], dim=1)  \n",
    "        output = self.bn_fc(output)\n",
    "        \n",
    "        output = self.fc(output)  # Use linear layer to combine them\n",
    "\n",
    "        prediction = self.out(output)  # [batch_size, vocab_size]\n",
    "\n",
    "        return prediction, hidden, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.018864Z",
     "iopub.status.busy": "2025-04-18T13:54:32.018680Z",
     "iopub.status.idle": "2025-04-18T13:54:32.036128Z",
     "shell.execute_reply": "2025-04-18T13:54:32.035633Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.018850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for batch in train_data_loader:\n",
    "#     t = batch\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.036916Z",
     "iopub.status.busy": "2025-04-18T13:54:32.036728Z",
     "iopub.status.idle": "2025-04-18T13:54:32.050267Z",
     "shell.execute_reply": "2025-04-18T13:54:32.049714Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.036902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, N_LAYERS, ENCODER_DROPOUT, BIDIRECTIONAL, DEVICE).to(DEVICE)\n",
    "# out, hidden = encoder(t[\"src_ids\"])\n",
    "# out.shape, hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.051233Z",
     "iopub.status.busy": "2025-04-18T13:54:32.050980Z",
     "iopub.status.idle": "2025-04-18T13:54:32.065481Z",
     "shell.execute_reply": "2025-04-18T13:54:32.064910Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.051212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# decoder = Decoder(HIDDEN_SIZE, VOCAB_SIZE, EMBEDDING_DIM, \"Luong\", DECODER_DROPOUT, N_LAYERS, BIDIRECTIONAL).to(DEVICE)\n",
    "# prediction, hidden, att = decoder(t[\"trg_ids\"][:, 0], hidden, out)\n",
    "# prediction.shape, hidden.shape, att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.066471Z",
     "iopub.status.busy": "2025-04-18T13:54:32.066215Z",
     "iopub.status.idle": "2025-04-18T13:54:32.079000Z",
     "shell.execute_reply": "2025-04-18T13:54:32.078449Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.066452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# seq2seq = Seq2Seq(encoder, decoder)\n",
    "# output = seq2seq(t[\"src_ids\"], t[\"trg_ids\"])\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.079825Z",
     "iopub.status.busy": "2025-04-18T13:54:32.079631Z",
     "iopub.status.idle": "2025-04-18T13:54:32.093774Z",
     "shell.execute_reply": "2025-04-18T13:54:32.093128Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.079805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.size(0)  # Get batch size from input\n",
    "        trg_len = target.size(1)\n",
    "        outputs = torch.zeros(batch_size, trg_len, OUTPUT_DIM).to(DEVICE)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(source)\n",
    "\n",
    "        input = target[:, 0]  # SOS\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, _ = self.decoder(\n",
    "                input, hidden, encoder_outputs\n",
    "            )\n",
    "\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1, device=DEVICE).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = target[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.094686Z",
     "iopub.status.busy": "2025-04-18T13:54:32.094446Z",
     "iopub.status.idle": "2025-04-18T13:54:32.714810Z",
     "shell.execute_reply": "2025-04-18T13:54:32.714213Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.094671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ATTENTION_TYPE = \"Bahdanau\"\n",
    "\n",
    "encoder =  Encoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, N_LAYERS, ENCODER_DROPOUT, BIDIRECTIONAL)\n",
    "decoder = Decoder(HIDDEN_SIZE, VOCAB_SIZE, EMBEDDING_DIM, ATTENTION_TYPE, DECODER_DROPOUT, N_LAYERS, BIDIRECTIONAL)\n",
    "seq2seq = Seq2Seq(encoder, decoder)\n",
    "encoder.to(DEVICE), decoder.to(DEVICE), seq2seq.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.715676Z",
     "iopub.status.busy": "2025-04-18T13:54:32.715468Z",
     "iopub.status.idle": "2025-04-18T13:54:32.721724Z",
     "shell.execute_reply": "2025-04-18T13:54:32.720966Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.715660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_metrics(train_losses, val_losses, bleu_scores):\n",
    "    epochs = len(train_losses)\n",
    "\n",
    "    # Plot Train Loss vs Validation Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(epochs), train_losses, label='Train Loss', color='blue')\n",
    "    plt.plot(range(epochs), val_losses, label='Validation Loss', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot BLEU score over epochs\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(epochs), bleu_scores, label='Validation BLEU Score', color='green')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('BLEU Score')\n",
    "    plt.title('Validation BLEU Score over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_bleu(predictions, targets):\n",
    "    \"\"\"\n",
    "    Compute the BLEU score for the predictions against the targets.\n",
    "    \n",
    "    Args:\n",
    "    predictions (list of list of int): Generated sequence of token IDs.\n",
    "    targets (list of list of int): Reference target sequence of token IDs.\n",
    "    \n",
    "    Returns:\n",
    "    float: BLEU score\n",
    "    \"\"\"\n",
    "    # Define a smoothing function\n",
    "    smoothing_function = SmoothingFunction().method4  # You can choose the method you prefer (e.g., method1, method2, etc.)\n",
    "\n",
    "    # BLEU expects lists of n-grams. Each target is a list of token IDs,\n",
    "    # and the predictions should be a list of token IDs too.\n",
    "    return corpus_bleu([[target] for target in targets], predictions, smoothing_function=smoothing_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.722618Z",
     "iopub.status.busy": "2025-04-18T13:54:32.722391Z",
     "iopub.status.idle": "2025-04-18T13:54:32.737173Z",
     "shell.execute_reply": "2025-04-18T13:54:32.736462Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.722602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_fn(model, train_loader, optimizer, criterion, clip, teacher_forcing_ratio=0.5, device='cuda'):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        source = batch['src_ids'].to(device)\n",
    "        target = batch['trg_ids'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(source, target, teacher_forcing_ratio)  # Get the model's output\n",
    "        \n",
    "        # Calculate loss (using CrossEntropy loss between the predicted and true target)\n",
    "        loss = criterion(output[1:].view(-1, output.size(-1)), target[1:].view(-1))  # Flatten for CE loss\n",
    "        \n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        # **Apply gradient clipping** to prevent exploding gradients\n",
    "        if clip:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()  # Update the model parameters\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "    return avg_train_loss\n",
    "\n",
    "def evaluate_fn(model, val_loader, criterion, device='cuda'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    epoch_val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for batch in val_loader:\n",
    "            # Move data to the specified device (CUDA or CPU)\n",
    "            source = batch['src_ids'].to(device)\n",
    "            target = batch['trg_ids'].to(device)\n",
    "\n",
    "            # Forward pass (no teacher forcing during evaluation)\n",
    "            output = model(source, target, teacher_forcing_ratio=0)  # teacher_forcing_ratio=0 during evaluation\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output[1:].view(-1, output.size(-1)), target[1:].view(-1))  # Flatten for CE loss\n",
    "\n",
    "            epoch_val_loss += loss.item()\n",
    "\n",
    "            # Store predictions and targets for BLEU score calculation (or other metrics)\n",
    "            pred = output.argmax(dim=-1)\n",
    "            val_predictions.extend(pred.cpu().numpy())\n",
    "            val_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # Calculate average validation loss for the epoch\n",
    "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Calculate BLEU score (you can replace this with other evaluation metrics if needed)\n",
    "    val_bleu_score = compute_bleu(val_predictions, val_targets)  # Assuming you have a BLEU function\n",
    "    print(f\"Validation BLEU Score: {val_bleu_score:.4f}\")\n",
    "    \n",
    "    return avg_val_loss, val_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.755991Z",
     "iopub.status.busy": "2025-04-18T13:54:32.755742Z",
     "iopub.status.idle": "2025-04-18T13:54:32.773262Z",
     "shell.execute_reply": "2025-04-18T13:54:32.772624Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.755977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.774686Z",
     "iopub.status.busy": "2025-04-18T13:54:32.773985Z",
     "iopub.status.idle": "2025-04-18T13:54:32.793440Z",
     "shell.execute_reply": "2025-04-18T13:54:32.792734Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.774661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, scheduler,\n",
    "                       n_epochs=1, teacher_forcing_ratio=0.5, device='cuda',\n",
    "                       start_epoch=0, train_losses=None, val_losses=None, bleu_scores=None,\n",
    "                       best_valid_loss=float(\"inf\")):\n",
    "\n",
    "    train_losses = train_losses or []\n",
    "    val_losses = val_losses or []\n",
    "    bleu_scores = bleu_scores or []\n",
    "\n",
    "    for epoch in tqdm(range(start_epoch, start_epoch+n_epochs), desc=\"Training Epochs\"):\n",
    "        print(f\"Epoch {epoch+1}/{start_epoch+n_epochs}\")\n",
    "\n",
    "        # Train the model\n",
    "        train_loss = train_fn(model, train_loader, optimizer, criterion,\n",
    "                              clip=1.0, teacher_forcing_ratio=teacher_forcing_ratio, device=device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate the model\n",
    "        val_loss, val_bleu_score = evaluate_fn(model, val_loader, criterion, device=device)\n",
    "        val_losses.append(val_loss)\n",
    "        bleu_scores.append(val_bleu_score)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_valid_loss:\n",
    "            best_valid_loss = val_loss\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'bleu_score': val_bleu_score\n",
    "                        }, 'checkpoint.pth')\n",
    "\n",
    "\n",
    "        # Print stats\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):.3f}\")\n",
    "        print(f\"Epoch {epoch+1} | Valid Loss: {val_loss:.3f} | Valid PPL: {np.exp(val_loss):.3f}\")\n",
    "        print(f\"Epoch {epoch+1} | Valid BLEU Score: {val_bleu_score:.3f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Plot\n",
    "    plot_metrics(train_losses, val_losses, bleu_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.794237Z",
     "iopub.status.busy": "2025-04-18T13:54:32.794043Z",
     "iopub.status.idle": "2025-04-18T13:54:32.811147Z",
     "shell.execute_reply": "2025-04-18T13:54:32.810540Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.794216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder, decoder).to(DEVICE)  # or however you instantiated it\n",
    "\n",
    "optimizer = optim.Adam(seq2seq.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_index)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "if checkpoint_path: \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    train_losses = [checkpoint['train_loss']]\n",
    "    val_losses = [checkpoint['val_loss']]\n",
    "    bleu_scores = [checkpoint['bleu_score']]\n",
    "    best_valid_loss = checkpoint['val_loss']  # assuming best = last val_loss\n",
    "    \n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "    best_valid_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.811946Z",
     "iopub.status.busy": "2025-04-18T13:54:32.811757Z",
     "iopub.status.idle": "2025-04-18T13:54:32.830763Z",
     "shell.execute_reply": "2025-04-18T13:54:32.830134Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.811932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Resume training\n",
    "train_and_evaluate(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    valid_data_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    scheduler,\n",
    "    n_epochs=10,\n",
    "    teacher_forcing_ratio=0.5,\n",
    "    device='cuda',\n",
    "    start_epoch=start_epoch,\n",
    "    train_losses=train_losses,  # start with previous\n",
    "    val_losses=val_losses,\n",
    "    bleu_scores=bleu_scores,\n",
    "    best_valid_loss=best_valid_loss # resume best val loss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:54:32.831757Z",
     "iopub.status.busy": "2025-04-18T13:54:32.831488Z",
     "iopub.status.idle": "2025-04-18T13:54:36.095499Z",
     "shell.execute_reply": "2025-04-18T13:54:36.094915Z",
     "shell.execute_reply.started": "2025-04-18T13:54:32.831734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = Seq2Seq(encoder, decoder).to(DEVICE)  # or however you instantiated it\n",
    "\n",
    "# # Load the weights\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T13:58:34.207296Z",
     "iopub.status.busy": "2025-04-18T13:58:34.206712Z",
     "iopub.status.idle": "2025-04-18T13:58:34.262990Z",
     "shell.execute_reply": "2025-04-18T13:58:34.262218Z",
     "shell.execute_reply.started": "2025-04-18T13:58:34.207275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def translate_sentence(sentence, model, en_tokenizer, vi_tokenizer, vi_id_to_token, device, max_output_length=MAX_LENGTH):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     with torch.no_grad():  # Disable gradient computation\n",
    "#         # Tokenize the input sentence (convert to token IDs)\n",
    "#         encoding = en_tokenizer.encode(sentence)\n",
    "        \n",
    "#         # Add <SOS> and <EOS> tokens to the input\n",
    "#         src_ids = [en_tokenizer.token_to_id(sos_token)] + encoding.ids + [en_tokenizer.token_to_id(eos_token)]\n",
    "        \n",
    "#         # Convert the source IDs to a tensor with shape [1, seq_len]\n",
    "#         tensor = torch.LongTensor(src_ids).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "\n",
    "#         # Pass the tensor through the encoder\n",
    "#         encoder_outputs, hidden = model.encoder(tensor)  # hidden, cell from LSTM encoder\n",
    "\n",
    "#         # Initialize input for the decoder with <SOS> token\n",
    "#         input_token = torch.LongTensor([vi_tokenizer.token_to_id(sos_token)]).to(device)  # [1]\n",
    "        \n",
    "#         outputs = [input_token.item()]  # Store the generated tokens\n",
    "#         for _ in range(max_output_length):\n",
    "            \n",
    "#             # Pass the current input token along with hidden and cell states to the decoder\n",
    "#             output, hidden, _ = model.decoder(input_token, hidden, encoder_outputs)\n",
    "\n",
    "#             # Get the predicted token (index of the highest probability)\n",
    "#             predicted_token = output.argmax(-1).item()\n",
    "#             # print(f\"Predicted Token ID: {predicted_token}\")\n",
    "#             # Append the predicted token to the output sequence\n",
    "#             outputs.append(predicted_token)\n",
    "\n",
    "#             # Update the input token for the next step (teacher-forcing is not used here)\n",
    "#             input_token = torch.LongTensor([predicted_token]).to(device)\n",
    "\n",
    "#             # If the decoder outputs <EOS>, stop the generation\n",
    "#             if predicted_token == vi_tokenizer.token_to_id(eos_token):\n",
    "#                 break\n",
    "        \n",
    "#         # Convert the predicted token IDs back to words\n",
    "#         translated_tokens = [vi_id_to_token[idx] for idx in outputs]\n",
    "        \n",
    "#     return translated_tokens\n",
    "\n",
    "# # Test translation on a single example\n",
    "\n",
    "# def en_vi(i):\n",
    "#     return train_data[i][\"en\"], train_data[i][\"vi\"]\n",
    "\n",
    "# sentence ,expected_translation = en_vi(9)\n",
    "# print(\"Source (English):\", sentence)\n",
    "# print(\"Expected Translation (Vietnamese):\", expected_translation)\n",
    "\n",
    "# # Run the translation function\n",
    "# translation = translate_sentence(sentence, model, en_tokenizer, vi_tokenizer, vi_id_to_token, DEVICE)\n",
    "# print(\"Model Translation:\", translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 310566,
     "modelInstanceId": 289825,
     "sourceId": 346940,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q evaluate\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport numpy as np\nimport pandas as pd\n\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torch\n\nfrom datasets import load_dataset\n\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nfrom torchsummary import summary\n\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom tqdm import tqdm\nimport evaluate","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:43:41.175741Z","iopub.execute_input":"2025-04-20T13:43:41.175960Z","iopub.status.idle":"2025-04-20T13:44:11.509499Z","shell.execute_reply.started":"2025-04-20T13:43:41.175936Z","shell.execute_reply":"2025-04-20T13:44:11.508908Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-04-20 13:43:57.159175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745156637.346567      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745156637.399109      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\nset_seed(42)  ","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:11.510173Z","iopub.execute_input":"2025-04-20T13:44:11.510701Z","iopub.status.idle":"2025-04-20T13:44:11.518143Z","shell.execute_reply.started":"2025-04-20T13:44:11.510677Z","shell.execute_reply":"2025-04-20T13:44:11.517509Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"ds = load_dataset(\"thainq107/iwslt2015-en-vi\")\ntrain_data, valid_data, test_data = ds[\"train\"], ds[\"validation\"], ds[\"test\"]\nen_lengths = [len(sentence) for sentence in train_data[\"en\"]]\nvi_lengths = [len(sentence) for sentence in train_data[\"vi\"]]\n\ncombined_lengths = en_lengths + vi_lengths\n\n# Calculate Q1, Q3, and IQR\nQ1 = np.percentile(combined_lengths, 25)\nQ3 = np.percentile(combined_lengths, 75)\nIQR = Q3 - Q1\n\nMAX_LENGTH = int(Q3 + 1.5*IQR)\nMAX_LENGTH = 50","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:11.519610Z","iopub.execute_input":"2025-04-20T13:44:11.519811Z","iopub.status.idle":"2025-04-20T13:44:14.517604Z","shell.execute_reply.started":"2025-04-20T13:44:11.519795Z","shell.execute_reply":"2025-04-20T13:44:14.516983Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/522 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4740f6e94b3146f09e3c7b236c3a7a80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb99827079fb4a72be964c1a9227b9e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/181k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ce9b9d2a1947d4909918ef552722df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/133317 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb8020aa9bb1443a85556f890cb85f71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fafc3bd864a44f7803dce2364977359"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f67e69fbcc43458996c613f2175edd14"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Define special tokens\nunk_token = \"[UNK]\"\npad_token = \"[PAD]\"\nsos_token = \"<sos>\"\neos_token = \"<eos>\"\nspecial_tokens = [unk_token, pad_token, sos_token, eos_token]\n\nen_tokenizer = Tokenizer(BPE(unk_token=unk_token))\nen_tokenizer.pre_tokenizer = Whitespace()\nen_trainer = BpeTrainer(special_tokens=special_tokens)\nen_tokenizer.train_from_iterator(train_data[\"en\"], trainer=en_trainer)\n\nvi_tokenizer = Tokenizer(BPE(unk_token=unk_token))\nvi_tokenizer.pre_tokenizer = Whitespace()\nvi_trainer = BpeTrainer(special_tokens=special_tokens)\nvi_tokenizer.train_from_iterator(train_data[\"vi\"], trainer=vi_trainer)\n\n# Build reverse lookup maps for later decoding (id -> token)\ndef id_to_token(tokenizer):\n    vocab = tokenizer.get_vocab()\n    return {v: k for k, v in vocab.items()}\n    \n# For decoder\nen_id_to_token = id_to_token(en_tokenizer)\nvi_id_to_token = id_to_token(vi_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:14.518271Z","iopub.execute_input":"2025-04-20T13:44:14.518456Z","iopub.status.idle":"2025-04-20T13:44:19.461519Z","shell.execute_reply.started":"2025-04-20T13:44:14.518441Z","shell.execute_reply":"2025-04-20T13:44:19.460755Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\n\n\n\n\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"VOCAB_SIZE = len(en_tokenizer.get_vocab())   \nOUTPUT_DIM = len(vi_tokenizer.get_vocab())    \nEMBEDDING_DIM = 256 \nBATCH_SIZE = 32\nHIDDEN_SIZE = 512            \nN_LAYERS = 2\nENCODER_DROPOUT = 0.2\nDECODER_DROPOUT = 0.2\nBIDIRECTIONAL = False\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:19.462322Z","iopub.execute_input":"2025-04-20T13:44:19.462569Z","iopub.status.idle":"2025-04-20T13:44:19.492626Z","shell.execute_reply.started":"2025-04-20T13:44:19.462542Z","shell.execute_reply":"2025-04-20T13:44:19.491833Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def tokenize(data, tokenizer, max_length=MAX_LENGTH):\n    encoding = tokenizer.encode(data)\n\n    ids = [tokenizer.token_to_id(sos_token)] + encoding.ids + [tokenizer.token_to_id(eos_token)]\n    return ids\n\ndef tokenize_and_numericalize(data, src_tokenizer, trg_tokenizer, max_length=MAX_LENGTH):\n    return {\"src_ids\": tokenize(data[\"en\"], src_tokenizer),\n            \"trg_ids\": tokenize(data[\"vi\"], trg_tokenizer)    \n           }\n\ntrain_data = train_data.map(lambda x: tokenize_and_numericalize(x, en_tokenizer, vi_tokenizer))\nvalid_data = valid_data.map(lambda x: tokenize_and_numericalize(x, en_tokenizer, vi_tokenizer))\ntest_data = test_data.map(lambda x: tokenize_and_numericalize(x, en_tokenizer, vi_tokenizer))","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:19.493508Z","iopub.execute_input":"2025-04-20T13:44:19.493821Z","iopub.status.idle":"2025-04-20T13:44:48.206183Z","shell.execute_reply.started":"2025-04-20T13:44:19.493802Z","shell.execute_reply":"2025-04-20T13:44:48.205500Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/133317 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89a32ad41d794b9d95cc08666a389aea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cdd8d1284414b1ab9b70dcd18758372"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c5366fd2254939b69cbadf89ea20b2"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def get_collate_fn(src_pad_index, trg_pad_index):\n    def collate_fn(batch):\n        batch_src_ids = [torch.tensor(example[\"src_ids\"]) for example in batch]\n        batch_trg_ids = [torch.tensor(example[\"trg_ids\"]) for example in batch]\n        \n        # Manually pad the sequences to MAX_LENGTH (ensure truncation if necessary)\n        for i in range(len(batch_src_ids)):\n            # Truncate source sequences that are longer than MAX_LENGTH\n            if len(batch_src_ids[i]) > MAX_LENGTH:\n                batch_src_ids[i] = batch_src_ids[i][:MAX_LENGTH]\n            # Pad source sequences that are shorter than MAX_LENGTH\n            elif len(batch_src_ids[i]) < MAX_LENGTH:\n                batch_src_ids[i] = torch.cat([batch_src_ids[i], torch.full((MAX_LENGTH - len(batch_src_ids[i]),), src_pad_index)])\n            \n            # Truncate target sequences that are longer than MAX_LENGTH\n            if len(batch_trg_ids[i]) > MAX_LENGTH:\n                batch_trg_ids[i] = batch_trg_ids[i][:MAX_LENGTH]\n            # Pad target sequences that are shorter than MAX_LENGTH\n            elif len(batch_trg_ids[i]) < MAX_LENGTH:\n                batch_trg_ids[i] = torch.cat([batch_trg_ids[i], torch.full((MAX_LENGTH - len(batch_trg_ids[i]),), trg_pad_index)])\n\n        # Stack all the sequences to create the batch\n        batch_src_ids = torch.stack(batch_src_ids)\n        batch_trg_ids = torch.stack(batch_trg_ids)\n\n        return {\"src_ids\": batch_src_ids, \"trg_ids\": batch_trg_ids}\n    \n    return collate_fn\n\n\ndef get_data_loader(dataset, batch_size, src_pad_index, trg_pad_index, shuffle=False):\n    collate_fn = get_collate_fn(src_pad_index, trg_pad_index)\n    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle)\n\n\nsrc_pad_index = en_tokenizer.token_to_id(pad_token)\ntrg_pad_index = vi_tokenizer.token_to_id(pad_token)\n\ntrain_data_loader = get_data_loader(train_data, BATCH_SIZE, src_pad_index, trg_pad_index, shuffle=True)\nvalid_data_loader = get_data_loader(valid_data, BATCH_SIZE, src_pad_index, trg_pad_index)\ntest_data_loader  = get_data_loader(test_data,  BATCH_SIZE, src_pad_index, trg_pad_index)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:48.207032Z","iopub.execute_input":"2025-04-20T13:44:48.207250Z","iopub.status.idle":"2025-04-20T13:44:48.215561Z","shell.execute_reply.started":"2025-04-20T13:44:48.207227Z","shell.execute_reply":"2025-04-20T13:44:48.214845Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# for batch in train_data_loader:\n#     print(batch[\"trg_ids\"])\n#     break","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:48.216293Z","iopub.execute_input":"2025-04-20T13:44:48.216628Z","iopub.status.idle":"2025-04-20T13:44:48.720183Z","shell.execute_reply.started":"2025-04-20T13:44:48.216598Z","shell.execute_reply":"2025-04-20T13:44:48.719457Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers=2, dropout_p=0.1, bidirectional=False, device='cuda'):\n        super(Encoder, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout_p)\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, hidden_size)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        \n        self.device = device\n        self.bidirectional = bidirectional\n        \n\n    def forward(self, x):\n        # x: [batch_size, seq_len]\n        x = x.to(self.device)\n        embedding = self.dropout(self.embedding(x))\n\n        out, (hidden, cell) = self.lstm(embedding)\n        out = self.layer_norm(out)\n\n        \n        if self.bidirectional:\n            hidden = hidden[-2,:,:] + hidden[-1,:,:]\n            out = self.fc(out)\n        else:\n            hidden = hidden[-1,:,:]\n        # out: [batch_size, seq_len, hidden_size]\n        # hidden: [batch_size, hidden_size]\n        return out, hidden\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:48.723926Z","iopub.execute_input":"2025-04-20T13:44:48.724414Z","iopub.status.idle":"2025-04-20T13:44:48.736833Z","shell.execute_reply.started":"2025-04-20T13:44:48.724393Z","shell.execute_reply":"2025-04-20T13:44:48.736106Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class BahdanauAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super(BahdanauAttention, self).__init__()\n       \n        self.Wa = nn.Linear(hidden_size, hidden_size)  # For the query (decoder hidden state)\n        self.Ua = nn.Linear(hidden_size, hidden_size)  # For the keys (encoder outputs)\n        self.Va = nn.Linear(hidden_size, 1)            # Final layer for the attention score\n\n    def forward(self, keys, query):\n\n        query_transformed = self.Wa(query).unsqueeze(1)  # Shape: [batch_size, 1, hidden_size]\n        keys_transformed = self.Ua(keys)  # Shape: [batch_size, seq_len, hidden_size]\n\n       \n        scores = self.Va(torch.tanh(query_transformed + keys_transformed))  # Shape: [batch_size, seq_len, 1]\n\n        scores = scores.squeeze(2)  # Shape: [batch_size, seq_len]\n\n        attention_weights = F.softmax(scores, dim=-1)  # Shape: [batch_size, seq_len]\n\n      \n        context_vector = torch.bmm(attention_weights.unsqueeze(1), keys)  # Shape: [batch_size, 1, hidden_size]\n\n        context_vector = context_vector.squeeze(1)  # Shape: [batch_size, hidden_size]\n\n        return context_vector, attention_weights","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:48.737619Z","iopub.execute_input":"2025-04-20T13:44:48.737849Z","iopub.status.idle":"2025-04-20T13:44:48.752360Z","shell.execute_reply.started":"2025-04-20T13:44:48.737826Z","shell.execute_reply":"2025-04-20T13:44:48.751547Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class LuongAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super(LuongAttention, self).__init__()\n        self.attn = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, encoder_outputs, hidden):\n        # encoder_outputs: [batch_size, seq_len, hidden_size]\n        # hidden: [batch_size, hidden_size]\n\n        # Reshape hidden to be [batch_size, hidden_size, 1]\n        hidden = hidden.unsqueeze(2)  # [batch_size, hidden_size, 1]\n\n        # Compute attention scores (dot product)\n        attn_energies = torch.bmm(encoder_outputs, hidden) / (self.attn.in_features ** 0.5)  # [batch_size, seq_len, 1], Scaled dot-product attention\n        attn_energies = attn_energies.squeeze(2)  # [batch_size, seq_len]\n\n        # Compute the attention weights (probabilities)\n        attention_weights = torch.softmax(attn_energies, dim=1)  # [batch_size, seq_len]\n\n        # Compute the weighted sum of encoder outputs (context vector)\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, hidden_size]\n        context = context.squeeze(1)  # [batch_size, hidden_size]\n\n        return context, attention_weights\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:48.753219Z","iopub.execute_input":"2025-04-20T13:44:48.753454Z","iopub.status.idle":"2025-04-20T13:44:48.773908Z","shell.execute_reply.started":"2025-04-20T13:44:48.753437Z","shell.execute_reply":"2025-04-20T13:44:48.773149Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, hidden_size, vocab_size, embedding_dim, attention_type, dropout_p=0.1, n_layers=1, bidirectional=False):\n        super(Decoder, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.n_layers = n_layers\n        self.n_directions = 2 if bidirectional else 1\n        \n        if attention_type == 'Luong':\n            self.attention = LuongAttention(hidden_size)\n        elif attention_type == 'Bahdanau':\n            self.attention = BahdanauAttention(hidden_size)\n        else:\n            raise ValueError(\"Unknown attention type\")\n\n        # Embedding layer\n        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n\n        # Dropout layer\n        self.dropout = nn.Dropout(dropout_p)\n\n        self.lstm = nn.LSTM(\n            input_size=self.embedding_dim,  \n            hidden_size=self.hidden_size,  \n            num_layers=n_layers,  \n            batch_first=True,  \n            bidirectional=bidirectional,  \n            dropout=dropout_p\n        )\n\n        self.fc = nn.Linear(hidden_size * self.n_directions + hidden_size , hidden_size)\n\n        # Output layer\n        self.out = nn.Linear(self.hidden_size, self.vocab_size)\n\n        # Batch normalization layers\n        self.bn_lstm = nn.BatchNorm1d(hidden_size * self.n_directions)\n        self.bn_fc = nn.BatchNorm1d(hidden_size + hidden_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        \"\"\"\n        input: [batch_size]\n        hidden: [batch_size, hidden_size]\n        encoder_outputs: [batch_size, seq_len, hidden_size]\n        \"\"\"\n\n        input = input.unsqueeze(1).to(DEVICE)\n        embedded = self.dropout(self.embedding(input))  # [batch_size, 1, embedding_dim]\n\n        if len(hidden.shape) == 2:\n            hidden = hidden.unsqueeze(0).repeat(self.n_layers * self.n_directions, 1, 1) # [, batch_size, hidden_size]\n        cell = torch.zeros_like(hidden).to(DEVICE)\n        \n        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # [batch_size, 1, hidden_size]\n\n        lstm_output = self.bn_lstm(lstm_output.squeeze(1))  # [batch_size, hidden_size]\n\n        if self.lstm.bidirectional:\n            hidden = hidden[-2, :, :] + hidden[-1, :, :]  \n            lstm_output = self.fc(lstm_output)\n        else:\n            hidden = hidden[-1, :, :]  \n\n        context, attention_weights = self.attention(encoder_outputs, hidden)\n\n        output = torch.cat([lstm_output, context], dim=1)  \n        output = self.bn_fc(output)\n        \n        output = self.fc(output)  # Use linear layer to combine them\n\n        prediction = self.out(output)  # [batch_size, vocab_size]\n\n        return prediction, hidden, attention_weights","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:52:21.463837Z","iopub.execute_input":"2025-04-20T13:52:21.464410Z","iopub.status.idle":"2025-04-20T13:52:21.473397Z","shell.execute_reply.started":"2025-04-20T13:52:21.464380Z","shell.execute_reply":"2025-04-20T13:52:21.472700Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# for batch in train_data_loader:\n#     t = batch\n#     break","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:48.798091Z","iopub.execute_input":"2025-04-20T13:44:48.798368Z","iopub.status.idle":"2025-04-20T13:44:48.845691Z","shell.execute_reply.started":"2025-04-20T13:44:48.798350Z","shell.execute_reply":"2025-04-20T13:44:48.845135Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, N_LAYERS, ENCODER_DROPOUT, BIDIRECTIONAL, DEVICE).to(DEVICE)\n# out, hidden = encoder(t[\"src_ids\"])\n# out.shape, hidden.shape","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:50:28.619357Z","iopub.execute_input":"2025-04-20T13:50:28.619727Z","iopub.status.idle":"2025-04-20T13:50:28.717412Z","shell.execute_reply.started":"2025-04-20T13:50:28.619698Z","shell.execute_reply":"2025-04-20T13:50:28.716701Z"},"trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 50, 512]), torch.Size([32, 512]))"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# decoder = Decoder(HIDDEN_SIZE, VOCAB_SIZE, EMBEDDING_DIM, \"Luong\", DECODER_DROPOUT, N_LAYERS, BIDIRECTIONAL).to(DEVICE)\n# prediction, hidden, att = decoder(t[\"trg_ids\"][:, 0], hidden, out)\n# prediction.shape, hidden.shape, att.shape","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:52:25.435887Z","iopub.execute_input":"2025-04-20T13:52:25.436148Z","iopub.status.idle":"2025-04-20T13:52:25.812967Z","shell.execute_reply.started":"2025-04-20T13:52:25.436129Z","shell.execute_reply":"2025-04-20T13:52:25.812252Z"},"trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 30000]), torch.Size([32, 512]), torch.Size([32, 50]))"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# seq2seq = Seq2Seq(encoder, decoder)\n# output = seq2seq(t[\"src_ids\"], t[\"trg_ids\"])\n# output.shape","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:52:37.370315Z","iopub.execute_input":"2025-04-20T13:52:37.370587Z","iopub.status.idle":"2025-04-20T13:52:37.603376Z","shell.execute_reply.started":"2025-04-20T13:52:37.370566Z","shell.execute_reply":"2025-04-20T13:52:37.602753Z"},"trusted":true},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 50, 30000])"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = source.size(0)  # Get batch size from input\n        trg_len = target.size(1)\n        outputs = torch.zeros(batch_size, trg_len, OUTPUT_DIM).to(DEVICE)\n\n        encoder_outputs, hidden = self.encoder(source)\n\n        input = target[:, 0]  # SOS\n\n        for t in range(1, trg_len):\n            output, hidden, _ = self.decoder(\n                input, hidden, encoder_outputs\n            )\n\n            outputs[:, t, :] = output\n            teacher_force = torch.rand(1, device=DEVICE).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = target[:, t] if teacher_force else top1\n\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:52:35.103518Z","iopub.execute_input":"2025-04-20T13:52:35.103792Z","iopub.status.idle":"2025-04-20T13:52:35.109752Z","shell.execute_reply.started":"2025-04-20T13:52:35.103772Z","shell.execute_reply":"2025-04-20T13:52:35.109139Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"ATTENTION_TYPE = \"Bahdanau\"\n\nencoder =  Encoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, N_LAYERS, ENCODER_DROPOUT, BIDIRECTIONAL)\ndecoder = Decoder(HIDDEN_SIZE, VOCAB_SIZE, EMBEDDING_DIM, ATTENTION_TYPE, DECODER_DROPOUT, N_LAYERS, BIDIRECTIONAL)\nseq2seq = Seq2Seq(encoder, decoder)\nencoder.to(DEVICE), decoder.to(DEVICE), seq2seq.to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:49.807841Z","iopub.status.idle":"2025-04-20T13:44:49.808118Z","shell.execute_reply.started":"2025-04-20T13:44:49.808004Z","shell.execute_reply":"2025-04-20T13:44:49.808017Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metrics(train_losses, val_losses, bleu_scores):\n    epochs = len(train_losses)\n\n    # Plot Train Loss vs Validation Loss\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(epochs), train_losses, label='Train Loss', color='blue')\n    plt.plot(range(epochs), val_losses, label='Validation Loss', color='red')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n\n    # Plot BLEU score over epochs\n    plt.subplot(1, 2, 2)\n    plt.plot(range(epochs), bleu_scores, label='Validation BLEU Score', color='green')\n    plt.xlabel('Epochs')\n    plt.ylabel('BLEU Score')\n    plt.title('Validation BLEU Score over Epochs')\n    plt.legend()\n\n    # Show the plots\n    plt.tight_layout()\n    plt.show()\n\n\ndef compute_bleu(predictions, targets):\n    \"\"\"\n    Compute the BLEU score for the predictions against the targets.\n    \n    Args:\n    predictions (list of list of int): Generated sequence of token IDs.\n    targets (list of list of int): Reference target sequence of token IDs.\n    \n    Returns:\n    float: BLEU score\n    \"\"\"\n    # Define a smoothing function\n    smoothing_function = SmoothingFunction().method4  # You can choose the method you prefer (e.g., method1, method2, etc.)\n\n    # BLEU expects lists of n-grams. Each target is a list of token IDs,\n    # and the predictions should be a list of token IDs too.\n    return corpus_bleu([[target] for target in targets], predictions, smoothing_function=smoothing_function)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:49.808991Z","iopub.status.idle":"2025-04-20T13:44:49.809268Z","shell.execute_reply.started":"2025-04-20T13:44:49.809120Z","shell.execute_reply":"2025-04-20T13:44:49.809133Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_fn(model, train_loader, optimizer, criterion, clip, teacher_forcing_ratio=0.5, device='cuda'):\n    model.train()  # Set model to training mode\n    epoch_train_loss = 0\n\n    for batch in train_loader:\n        source = batch['src_ids'].to(device)\n        target = batch['trg_ids'].to(device)\n\n        optimizer.zero_grad()  # Clear previous gradients\n        \n        # Forward pass\n        output = model(source, target, teacher_forcing_ratio)  # Get the model's output\n        \n        # Calculate loss (using CrossEntropy loss between the predicted and true target)\n        loss = criterion(output[1:].view(-1, output.size(-1)), target[1:].view(-1))  # Flatten for CE loss\n        \n        loss.backward()  # Backpropagation\n        \n        # **Apply gradient clipping** to prevent exploding gradients\n        if clip:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()  # Update the model parameters\n        \n        epoch_train_loss += loss.item()\n\n    # Calculate average training loss for the epoch\n    avg_train_loss = epoch_train_loss / len(train_loader)\n    print(f\"Training Loss: {avg_train_loss:.4f}\")\n    return avg_train_loss\n\ndef evaluate_fn(model, val_loader, criterion, device='cuda'):\n    model.eval()  # Set model to evaluation mode\n    epoch_val_loss = 0\n    val_predictions = []\n    val_targets = []\n\n    with torch.no_grad():  # No need to compute gradients during evaluation\n        for batch in val_loader:\n            # Move data to the specified device (CUDA or CPU)\n            source = batch['src_ids'].to(device)\n            target = batch['trg_ids'].to(device)\n\n            # Forward pass (no teacher forcing during evaluation)\n            output = model(source, target, teacher_forcing_ratio=0)  # teacher_forcing_ratio=0 during evaluation\n            \n            # Calculate loss\n            loss = criterion(output[1:].view(-1, output.size(-1)), target[1:].view(-1))  # Flatten for CE loss\n\n            epoch_val_loss += loss.item()\n\n            # Store predictions and targets for BLEU score calculation (or other metrics)\n            pred = output.argmax(dim=-1)\n            val_predictions.extend(pred.cpu().numpy())\n            val_targets.extend(target.cpu().numpy())\n\n    # Calculate average validation loss for the epoch\n    avg_val_loss = epoch_val_loss / len(val_loader)\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n\n    # Calculate BLEU score (you can replace this with other evaluation metrics if needed)\n    val_bleu_score = compute_bleu(val_predictions, val_targets)  # Assuming you have a BLEU function\n    print(f\"Validation BLEU Score: {val_bleu_score:.4f}\")\n    \n    return avg_val_loss, val_bleu_score","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:49.810053Z","iopub.status.idle":"2025-04-20T13:44:49.810622Z","shell.execute_reply.started":"2025-04-20T13:44:49.810515Z","shell.execute_reply":"2025-04-20T13:44:49.810526Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ncheckpoint_path = \"\"","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:49.811572Z","iopub.status.idle":"2025-04-20T13:44:49.811869Z","shell.execute_reply.started":"2025-04-20T13:44:49.811705Z","shell.execute_reply":"2025-04-20T13:44:49.811720Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, scheduler,\n                       n_epochs=1, teacher_forcing_ratio=0.5, device='cuda',\n                       start_epoch=0, train_losses=None, val_losses=None, bleu_scores=None,\n                       best_valid_loss=float(\"inf\")):\n\n    train_losses = train_losses or []\n    val_losses = val_losses or []\n    bleu_scores = bleu_scores or []\n\n    for epoch in tqdm(range(start_epoch, start_epoch+n_epochs), desc=\"Training Epochs\"):\n        print(f\"Epoch {epoch+1}/{start_epoch+n_epochs}\")\n\n        # Train the model\n        train_loss = train_fn(model, train_loader, optimizer, criterion,\n                              clip=1.0, teacher_forcing_ratio=teacher_forcing_ratio, device=device)\n        train_losses.append(train_loss)\n\n        # Evaluate the model\n        val_loss, val_bleu_score = evaluate_fn(model, val_loader, criterion, device=device)\n        val_losses.append(val_loss)\n        bleu_scores.append(val_bleu_score)\n\n        # Save best model\n        if val_loss < best_valid_loss:\n            best_valid_loss = val_loss\n            torch.save({'epoch': epoch,\n                        'model_state_dict': model.state_dict(),\n                        'optimizer_state_dict': optimizer.state_dict(),\n                        'scheduler_state_dict': scheduler.state_dict(),\n                        'train_loss': train_loss,\n                        'val_loss': val_loss,\n                        'bleu_score': val_bleu_score\n                        }, 'checkpoint.pth')\n\n\n        # Print stats\n        print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):.3f}\")\n        print(f\"Epoch {epoch+1} | Valid Loss: {val_loss:.3f} | Valid PPL: {np.exp(val_loss):.3f}\")\n        print(f\"Epoch {epoch+1} | Valid BLEU Score: {val_bleu_score:.3f}\")\n\n        scheduler.step(val_loss)\n\n    # Plot\n    plot_metrics(train_losses, val_losses, bleu_scores)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:49.813321Z","iopub.status.idle":"2025-04-20T13:44:49.813571Z","shell.execute_reply.started":"2025-04-20T13:44:49.813457Z","shell.execute_reply":"2025-04-20T13:44:49.813470Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Seq2Seq(encoder, decoder).to(DEVICE)  # or however you instantiated it\n\noptimizer = optim.Adam(seq2seq.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss(ignore_index=src_pad_index)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\n\n\nif checkpoint_path: \n    checkpoint = torch.load(checkpoint_path)\n    \n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    \n    start_epoch = checkpoint['epoch'] + 1\n    train_losses = [checkpoint['train_loss']]\n    val_losses = [checkpoint['val_loss']]\n    bleu_scores = [checkpoint['bleu_score']]\n    best_valid_loss = checkpoint['val_loss']  # assuming best = last val_loss\n    \n    print(f\"Resuming from epoch {start_epoch}\")\nelse:\n    start_epoch = 0\n    train_losses = []\n    val_losses = []\n    bleu_scores = []\n    best_valid_loss = float(\"inf\")","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:49.814384Z","iopub.status.idle":"2025-04-20T13:44:49.814696Z","shell.execute_reply.started":"2025-04-20T13:44:49.814546Z","shell.execute_reply":"2025-04-20T13:44:49.814560Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Resume training\ntrain_and_evaluate(\n    model,\n    train_data_loader,\n    valid_data_loader,\n    optimizer,\n    criterion,\n    scheduler,\n    n_epochs=10,\n    teacher_forcing_ratio=0.5,\n    device='cuda',\n    start_epoch=start_epoch,\n    train_losses=train_losses,  # start with previous\n    val_losses=val_losses,\n    bleu_scores=bleu_scores,\n    best_valid_loss=best_valid_loss # resume best val loss\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:49.815745Z","iopub.status.idle":"2025-04-20T13:44:49.816043Z","shell.execute_reply.started":"2025-04-20T13:44:49.815894Z","shell.execute_reply":"2025-04-20T13:44:49.815908Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = Seq2Seq(encoder, decoder).to(DEVICE)  # or however you instantiated it\n\n# # Load the weights\n# checkpoint = torch.load(checkpoint_path)\n    \n# model.load_state_dict(checkpoint['model_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:49.817391Z","iopub.status.idle":"2025-04-20T13:44:49.817634Z","shell.execute_reply.started":"2025-04-20T13:44:49.817520Z","shell.execute_reply":"2025-04-20T13:44:49.817532Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def translate_sentence(sentence, model, en_tokenizer, vi_tokenizer, vi_id_to_token, device, max_output_length=MAX_LENGTH):\n#     model.eval()  # Set the model to evaluation mode\n#     with torch.no_grad():  # Disable gradient computation\n#         # Tokenize the input sentence (convert to token IDs)\n#         encoding = en_tokenizer.encode(sentence)\n        \n#         # Add <SOS> and <EOS> tokens to the input\n#         src_ids = [en_tokenizer.token_to_id(sos_token)] + encoding.ids + [en_tokenizer.token_to_id(eos_token)]\n        \n#         # Convert the source IDs to a tensor with shape [1, seq_len]\n#         tensor = torch.LongTensor(src_ids).unsqueeze(0).to(device)  # [1, seq_len]\n\n#         # Pass the tensor through the encoder\n#         encoder_outputs, hidden = model.encoder(tensor)  # hidden, cell from LSTM encoder\n\n#         # Initialize input for the decoder with <SOS> token\n#         input_token = torch.LongTensor([vi_tokenizer.token_to_id(sos_token)]).to(device)  # [1]\n        \n#         outputs = [input_token.item()]  # Store the generated tokens\n#         for _ in range(max_output_length):\n            \n#             # Pass the current input token along with hidden and cell states to the decoder\n#             output, hidden, _ = model.decoder(input_token, hidden, encoder_outputs)\n\n#             # Get the predicted token (index of the highest probability)\n#             predicted_token = output.argmax(-1).item()\n#             # print(f\"Predicted Token ID: {predicted_token}\")\n#             # Append the predicted token to the output sequence\n#             outputs.append(predicted_token)\n\n#             # Update the input token for the next step (teacher-forcing is not used here)\n#             input_token = torch.LongTensor([predicted_token]).to(device)\n\n#             # If the decoder outputs <EOS>, stop the generation\n#             if predicted_token == vi_tokenizer.token_to_id(eos_token):\n#                 break\n        \n#         # Convert the predicted token IDs back to words\n#         translated_tokens = [vi_id_to_token[idx] for idx in outputs]\n        \n#     return translated_tokens\n\n# # Test translation on a single example\n\n# def en_vi(i):\n#     return train_data[i][\"en\"], train_data[i][\"vi\"]\n\n# sentence ,expected_translation = en_vi(9)\n# print(\"Source (English):\", sentence)\n# print(\"Expected Translation (Vietnamese):\", expected_translation)\n\n# # Run the translation function\n# translation = translate_sentence(sentence, model, en_tokenizer, vi_tokenizer, vi_id_to_token, DEVICE)\n# print(\"Model Translation:\", translation)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T13:44:49.819009Z","iopub.status.idle":"2025-04-20T13:44:49.819265Z","shell.execute_reply.started":"2025-04-20T13:44:49.819131Z","shell.execute_reply":"2025-04-20T13:44:49.819141Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}